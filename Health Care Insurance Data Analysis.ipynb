{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bbae33ce-30d4-416b-b299-88c57bea18a0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Importing SparkSession and initializing Spark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"HealthInsuranceDataAnalysis\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "763e754d-3948-45b9-9f2b-5887b7d4b991",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+--------------+------------------+--------------+--------------+------------+-----------+\n|Patient_id|Patient_name|patient_gender|patient_birth_date| patient_phone|  disease_name|        city|hospital_id|\n+----------+------------+--------------+------------------+--------------+--------------+------------+-----------+\n|    187158|      Harbir|        Female|        1924-06-30|+91 0112009318|  Galactosemia|    Rourkela|      H1001|\n|    112766|    Brahmdev|        Female|        1948-12-20|+91 1727749552|Bladder cancer|Tiruvottiyur|      H1016|\n|    199252|     Ujjawal|          Male|        1980-04-16|+91 8547451606| Kidney cancer|   Berhampur|      H1009|\n|    133424|     Ballari|        Female|        1969-09-25|+91 0106026841|       Suicide|Bihar Sharif|      H1017|\n|    172579|     Devnath|        Female|        1946-05-01|+91 1868774631|  Food allergy| Bidhannagar|      H1019|\n+----------+------------+--------------+------------------+--------------+--------------+------------+-----------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Reading the raw data from DBFS\n",
    "# Datset: Patients_records.csv\n",
    "patients_df = spark.read.csv(\"dbfs:/FileStore/tables/Patient_records.csv\", header=True, inferSchema=True)\n",
    "patients_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f7c7534-5263-4603-a18a-b8b3a36cff9f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-----------+-----------+----------+------+--------------+-------+------------+--------+---------+--------+----------+----------+\n|   sub _id|first_name|  last_name|     Street|Birth_date|Gender|         Phone|Country|        City|Zip Code|Subgrp_id|Elig_ind|  eff_date| term_date|\n+----------+----------+-----------+-----------+----------+------+--------------+-------+------------+--------+---------+--------+----------+----------+\n|SUBID10000|    Harbir|Vishwakarma| Baria Marg|1924-06-30|Female|+91 0112009318|  India|    Rourkela|  767058|     S107|       Y|1944-06-30|1954-01-14|\n|SUBID10001|  Brahmdev|     Sonkar|  Lala Marg|1948-12-20|Female|+91 1727749552|  India|Tiruvottiyur|   34639|     S105|       Y|1968-12-20|1970-05-16|\n|SUBID10002|   Ujjawal|       Devi|Mammen Zila|1980-04-16|  Male|+91 8547451606|  India|   Berhampur|  914455|     S106|       N|2000-04-16|2008-05-04|\n|SUBID10003|   Ballari|     Mishra| Sahni Zila|1969-09-25|Female|+91 0106026841|  India|Bihar Sharif|   91481|     S104|       N|1989-09-25|1995-06-05|\n|SUBID10004|   Devnath|  Srivastav| Magar Zila|1946-05-01|Female|+91 1868774631|  India| Bidhannagar|  531742|     S110|       N|1966-05-01|1970-12-09|\n+----------+----------+-----------+-----------+----------+------+--------------+-------+------------+--------+---------+--------+----------+----------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Dataset: subscriber.csv\n",
    "subscriber_df = spark.read.csv(\"dbfs:/FileStore/tables/subscriber.csv\", header=True, inferSchema=True)\n",
    "subscriber_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7f6e9f1-99f8-454d-99cf-288c593afbbf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----------+------------+----------+--------+----------------+--------------+----------+\n|Claim_Or_Rejected|    SUB_ID|claim_amount|claim_date|claim_id|      claim_type|  disease_name|patient_id|\n+-----------------+----------+------------+----------+--------+----------------+--------------+----------+\n|                N| SUBID1000|       79874|1949-03-14|       0| claims of value|  Galactosemia|    187158|\n|              NaN|SUBID10001|      151142|1970-03-16|       1|claims of policy|Bladder cancer|    112766|\n|              NaN|SUBID10002|       59924|2008-02-03|       2| claims of value| Kidney cancer|    199252|\n|              NaN|SUBID10003|      143120|1995-02-08|       3|  claims of fact|       Suicide|    133424|\n|                Y|SUBID10004|      168634|1967-05-23|       4| claims of value|  Food allergy|    172579|\n+-----------------+----------+------------+----------+--------+----------------+--------------+----------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Dataset: claims.json\n",
    "claims_df = spark.read.json(\"dbfs:/FileStore/tables/claims.json\")\n",
    "claims_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4b40117-d1b7-4258-a295-c4f544c69862",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+\n|SubGrp_ID|Grp_Id|\n+---------+------+\n|     S101|GRP101|\n|     S101|GRP105|\n|     S102|GRP110|\n|     S102|GRP150|\n|     S102|GRP136|\n+---------+------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Dataset: grpsubgrp.csv\n",
    "group_subgroup_df = spark.read.csv(\"dbfs:/FileStore/tables/grpsubgrp.csv\", header=True, inferSchema=True)\n",
    "group_subgroup_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92b0ab54-67be-49ad-a9a3-1ffbf249d8fd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+------------+\n|SubGrpID| Disease_ID|Disease_name|\n+--------+-----------+------------+\n|    S101|     110001|    Beriberi|\n|    S101|     110002|      Scurvy|\n|    S101|     110003|      Goitre|\n|    S101|     110004|Osteoporosis|\n|    S101|     110005|     Rickets|\n+--------+-----------+------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Dataset: disease.csv\n",
    "disease_df = spark.read.csv(\"dbfs:/FileStore/tables/disease.csv\", header=True, inferSchema=True)\n",
    "disease_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5cbdf02b-3ee8-4e88-856a-2eea9478686a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+---------------+\n|SubGrp_id|        SubGrp_Name|Monthly_Premium|\n+---------+-------------------+---------------+\n|     S101|Deficiency Diseases|           3000|\n|     S102|           Accident|           1000|\n|     S103|         Physiology|           2000|\n|     S104|            Therapy|           1500|\n|     S105|          Allergies|           2300|\n+---------+-------------------+---------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Dataset: subgroup.csv\n",
    "subgroup_df = spark.read.csv(\"dbfs:/FileStore/tables/subgroup.csv\", header=True, inferSchema=True)\n",
    "subgroup_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "303911f6-d473-4a69-b799-6dd652f17c1e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+----------+----------+-------+\n|Hospital_id|       Hospital_name|      city|     state|country|\n+-----------+--------------------+----------+----------+-------+\n|      H1000|All India Institu...| New Delhi|       NaN|  India|\n|      H1001|Medanta The Medicity|   Gurgaon|   Haryana|  India|\n|      H1002|The Christian Med...|   Vellore|Tamil Nadu|  India|\n|      H1003|PGIMER - Postgrad...|Chandigarh|   Haryana|  India|\n|      H1004|Apollo Hospital -...|   Chennai|Tamil Nadu|  India|\n+-----------+--------------------+----------+----------+-------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Dataset: hospital.csv\n",
    "hospital_df = spark.read.csv(\"dbfs:/FileStore/tables/hospital.csv\", header=True, inferSchema=True)\n",
    "hospital_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90e8cfee-bf2c-475b-b97b-dadc94b77750",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------+-------+------+--------------------+--------+------+----+\n|Country|premium_written|zipcode|Grp_Id|            Grp_Name|Grp_Type|  city|year|\n+-------+---------------+-------+------+--------------------+--------+------+----+\n|  India|          72000| 482018|GRP101|Life Insurance Co...|   Govt.|Mumbai|1956|\n|  India|          45000| 482049|GRP102|HDFC Standard Lif...| Private|Mumbai|2000|\n|  India|          64000| 482030|GRP103|Max Life Insuranc...| Private| Delhi|2000|\n|  India|          59000| 482028|GRP104|ICICI Prudential ...| Private|Mumbai|2000|\n|  India|          37000| 482014|GRP105|Kotak Mahindra Li...| Private|Mumbai|2001|\n+-------+---------------+-------+------+--------------------+--------+------+----+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Dataset: group.csv\n",
    "group_df = spark.read.csv(\"dbfs:/FileStore/tables/group.csv\", header=True, inferSchema=True)\n",
    "group_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "727faea4-7169-4210-baf2-e7c3c64383cb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+--------------+------------------+-------------+------------+----+-----------+\n|Patient_id|Patient_name|patient_gender|patient_birth_date|patient_phone|disease_name|city|hospital_id|\n+----------+------------+--------------+------------------+-------------+------------+----+-----------+\n|         0|          17|             0|                 0|            2|           0|   0|          0|\n+----------+------------+--------------+------------------+-------------+------------+----+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "# Data Cleaning\n",
    "from pyspark.sql.functions import col, count, when\n",
    "\n",
    "# Checking and counting null values for each column\n",
    "# Dataset: Patients_records.csv\n",
    "patients_null_count = patients_df.select([count(when(col(c).isNull(), c)).alias(c) for c in patients_df.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f1745f4-deab-4f4d-bc2f-5743e214ed96",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+---------+------+----------+------+-----+-------+----+--------+---------+--------+--------+---------+\n|sub _id|first_name|last_name|Street|Birth_date|Gender|Phone|Country|City|Zip Code|Subgrp_id|Elig_ind|eff_date|term_date|\n+-------+----------+---------+------+----------+------+-----+-------+----+--------+---------+--------+--------+---------+\n|      0|        27|        0|     0|         0|     0|    3|      0|   0|       0|        2|       4|       0|        0|\n+-------+----------+---------+------+----------+------+-----+-------+----+--------+---------+--------+--------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "# Dataset: subscriber.csv\n",
    "subscriber_null_count = subscriber_df.select([count(when(col(c).isNull(), c)).alias(c) for c in subscriber_df.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b307d172-17da-4d6f-afc4-858e8e2fe18f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------+------------+----------+--------+----------+------------+----------+\n|Claim_Or_Rejected|SUB_ID|claim_amount|claim_date|claim_id|claim_type|disease_name|patient_id|\n+-----------------+------+------------+----------+--------+----------+------------+----------+\n|                0|     0|           0|         0|       0|         0|           0|         0|\n+-----------------+------+------------+----------+--------+----------+------------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "# Dataset: claims.json\n",
    "claims_null_count = claims_df.select([count(when(col(c).isNull(), c)).alias(c) for c in claims_df.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fee71b2a-2beb-4e67-ac86-d9974dd8e28d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+\n|SubGrp_ID|Grp_Id|\n+---------+------+\n|        0|     0|\n+---------+------+\n\n"
     ]
    }
   ],
   "source": [
    "# Dataset: grpsubgrp.csv\n",
    "group_subgroup_null_count = group_subgroup_df.select([count(when(col(c).isNull(), c)).alias(c) for c in group_subgroup_df.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0486ad9d-ce42-4e9d-bd9a-502be112b4e6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+--------------+------------------+--------------+----------------+------------+-----------+\n|Patient_id|Patient_name|patient_gender|patient_birth_date| patient_phone|    disease_name|        city|hospital_id|\n+----------+------------+--------------+------------------+--------------+----------------+------------+-----------+\n|    187158|      Harbir|        Female|        1924-06-30|+91 0112009318|    Galactosemia|    Rourkela|      H1001|\n|    112766|    Brahmdev|        Female|        1948-12-20|+91 1727749552|  Bladder cancer|Tiruvottiyur|      H1016|\n|    199252|     Ujjawal|          Male|        1980-04-16|+91 8547451606|   Kidney cancer|   Berhampur|      H1009|\n|    133424|     Ballari|        Female|        1969-09-25|+91 0106026841|         Suicide|Bihar Sharif|      H1017|\n|    172579|     Devnath|        Female|        1946-05-01|+91 1868774631|    Food allergy| Bidhannagar|      H1019|\n|    171320|       Atasi|          Male|        1967-10-02|+91 9747336855|        Whiplash|    Amravati|      H1013|\n|    107794|      Manish|          Male|        1967-06-06|+91 4354294043|      Sunbathing|      Panvel|      H1004|\n|    130339|       Aakar|        Female|        1925-03-05|+91 2777633911|Drug consumption|Bihar Sharif|      H1000|\n|    110377|     Gurudas|          Male|        1945-05-06|+91 1232859381|          Dengue|   Kamarhati|      H1001|\n|    149367|          NA|          Male|        1925-06-12|+91 1780763280|    Head banging|   Bangalore|      H1013|\n+----------+------------+--------------+------------------+--------------+----------------+------------+-----------+\nonly showing top 10 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Replacing the null values for specific columns with \"NA\". \n",
    "# There is null values for Patients_records.csv and subscriber.csv datasets\n",
    "# Dataset: Patients_records.csv\n",
    "patients_df = patients_df.fillna(\"NA\")\n",
    "patients_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1c5cb75-ef38-422c-9ee7-ae514ab356e0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-----------+-----------------+----------+------+--------------+-------+------------+--------+---------+--------+----------+----------+\n|   sub _id|first_name|  last_name|           Street|Birth_date|Gender|         Phone|Country|        City|Zip Code|Subgrp_id|Elig_ind|  eff_date| term_date|\n+----------+----------+-----------+-----------------+----------+------+--------------+-------+------------+--------+---------+--------+----------+----------+\n|SUBID10000|    Harbir|Vishwakarma|       Baria Marg|1924-06-30|Female|+91 0112009318|  India|    Rourkela|  767058|     S107|       Y|1944-06-30|1954-01-14|\n|SUBID10001|  Brahmdev|     Sonkar|        Lala Marg|1948-12-20|Female|+91 1727749552|  India|Tiruvottiyur|   34639|     S105|       Y|1968-12-20|1970-05-16|\n|SUBID10002|   Ujjawal|       Devi|      Mammen Zila|1980-04-16|  Male|+91 8547451606|  India|   Berhampur|  914455|     S106|       N|2000-04-16|2008-05-04|\n|SUBID10003|   Ballari|     Mishra|       Sahni Zila|1969-09-25|Female|+91 0106026841|  India|Bihar Sharif|   91481|     S104|       N|1989-09-25|1995-06-05|\n|SUBID10004|   Devnath|  Srivastav|       Magar Zila|1946-05-01|Female|+91 1868774631|  India| Bidhannagar|  531742|     S110|       N|1966-05-01|1970-12-09|\n|SUBID10005|     Atasi|       Seth|     Khatri Nagar|1967-10-02|  Male|+91 9747336855|  India|    Amravati|  229062|     S104|       Y|1987-10-02|1995-02-13|\n| SUBID1006|    Manish|     Maurya|Swaminathan Chowk|1967-06-06|  Male|+91 4354294043|  India|      Panvel|  438733|     S109|      NA|1987-06-06|1995-03-21|\n|SUBID10007|     Aakar|      Yadav|            Swamy|1925-03-05|Female|+91 2777633911|  India|Bihar Sharif|  535907|     S104|       N|1945-03-05|1946-11-07|\n|SUBID10008|   Gurudas|      Gupta|      Sarin Nagar|1945-05-06|  Male|+91 1232859381|  India|   Kamarhati|  933226|     S103|       Y|1965-05-06|1970-09-16|\n|SUBID10009|        NA|      Gupta|    Thakur Circle|1925-06-12|  Male|+91 1780763280|  India|   Bangalore|  957469|     S105|       Y|1945-06-12|1953-08-30|\n+----------+----------+-----------+-----------------+----------+------+--------------+-------+------------+--------+---------+--------+----------+----------+\nonly showing top 10 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Dataset: subscriber.csv\n",
    "subscriber_df = subscriber_df.fillna(\"NA\")\n",
    "subscriber_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71575651-b337-43a8-90ca-551936fab45f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patients duplicates: 0\n"
     ]
    }
   ],
   "source": [
    "# Checking for duplicate records\n",
    "# If there are duplicates, then drop duplicates\n",
    "# Dataset: Patients_records.csv\n",
    "patients_duplicate_count = patients_df.groupBy(patients_df.columns).count().filter(\"count > 1\").count()\n",
    "print(f\"Patients duplicates: {patients_duplicate_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27c5a6b6-0864-4016-b2fb-131c0a314ec4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subscriber duplicates: 0\n"
     ]
    }
   ],
   "source": [
    "# Dataset: subscriber.csv\n",
    "subscriber_duplicate_count = subscriber_df.groupBy(subscriber_df.columns).count().filter(\"count > 1\").count()\n",
    "print(f\"Subscriber duplicates: {subscriber_duplicate_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9feda814-b21d-427c-b5d0-e2985ae76f21",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claims duplicates: 0\n"
     ]
    }
   ],
   "source": [
    "# Dataset: claims.json\n",
    "claims_duplicate_count = claims_df.groupBy(claims_df.columns).count().filter(\"count > 1\").count()\n",
    "print(f\"Claims duplicates: {claims_duplicate_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bcdcdb0c-4bc6-41f3-9995-ba697a904870",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group_Subgroup duplicates: 0\n"
     ]
    }
   ],
   "source": [
    "# Dataset: grpsubgrp.csv\n",
    "group_subgroup_duplicate_count = group_subgroup_df.groupBy(group_subgroup_df.columns).count().filter(\"count > 1\").count()\n",
    "print(f\"Group_Subgroup duplicates: {group_subgroup_duplicate_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a72d9e11-200c-453f-8d6e-0f2f367d4568",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+--------------+------------------+--------------+--------------+------------+-----------+\n|Patient_id|Patient_name|patient_gender|patient_birth_date| patient_phone|  disease_name|        city|hospital_id|\n+----------+------------+--------------+------------------+--------------+--------------+------------+-----------+\n|    112766|    Brahmdev|        Female|        1948-12-20|+91 1727749552|Bladder cancer|Tiruvottiyur|      H1016|\n|    199252|     Ujjawal|          Male|        1980-04-16|+91 8547451606| Kidney cancer|   Berhampur|      H1009|\n|    133424|     Ballari|        Female|        1969-09-25|+91 0106026841|       Suicide|Bihar Sharif|      H1017|\n|    172579|     Devnath|        Female|        1946-05-01|+91 1868774631|  Food allergy| Bidhannagar|      H1019|\n|    171320|       Atasi|          Male|        1967-10-02|+91 9747336855|      Whiplash|    Amravati|      H1013|\n+----------+------------+--------------+------------------+--------------+--------------+------------+-----------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Saving cleaned data back to DBFS \n",
    "# Dataset: Patients_records\n",
    "patients_df.write.format(\"csv\").mode(\"overwrite\").save(\"dbfs:/FileStore/tables/Cleaned_Patients_records.csv\")\n",
    "\n",
    "# Defining the correct column names\n",
    "columns = [\"Patient_id\", \"Patient_name\", \"patient_gender\", \"patient_birth_date\", \"patient_phone\", \"disease_name\", \"city\", \"hospital_id\"]\n",
    "\n",
    "# Reading the cleaned patients data from DBFS\n",
    "cleaned_patients_df = spark.read.csv(\"dbfs:/FileStore/tables/Cleaned_Patients_records.csv\", header=True, inferSchema=True).toDF(*columns)\n",
    "\n",
    "# Displaying the first few records to confirm the data\n",
    "cleaned_patients_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd05b0ad-4444-463f-98a0-45ab73f37918",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+---------+------------+----------+------+--------------+-------+------------+--------+---------+--------+----------+----------+\n|    sub_id|first_name|last_name|      Street|Birth_date|Gender|         Phone|Country|        City|Zip_Code|Subgrp_id|Elig_ind|  eff_date| term_date|\n+----------+----------+---------+------------+----------+------+--------------+-------+------------+--------+---------+--------+----------+----------+\n|SUBID10001|  Brahmdev|   Sonkar|   Lala Marg|1948-12-20|Female|+91 1727749552|  India|Tiruvottiyur|   34639|     S105|       Y|1968-12-20|1970-05-16|\n|SUBID10002|   Ujjawal|     Devi| Mammen Zila|1980-04-16|  Male|+91 8547451606|  India|   Berhampur|  914455|     S106|       N|2000-04-16|2008-05-04|\n|SUBID10003|   Ballari|   Mishra|  Sahni Zila|1969-09-25|Female|+91 0106026841|  India|Bihar Sharif|   91481|     S104|       N|1989-09-25|1995-06-05|\n|SUBID10004|   Devnath|Srivastav|  Magar Zila|1946-05-01|Female|+91 1868774631|  India| Bidhannagar|  531742|     S110|       N|1966-05-01|1970-12-09|\n|SUBID10005|     Atasi|     Seth|Khatri Nagar|1967-10-02|  Male|+91 9747336855|  India|    Amravati|  229062|     S104|       Y|1987-10-02|1995-02-13|\n+----------+----------+---------+------------+----------+------+--------------+-------+------------+--------+---------+--------+----------+----------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Dataset: subscriber.csv\n",
    "subscriber_df.write.format(\"csv\").mode(\"overwrite\").save(\"dbfs:/FileStore/tables/Cleaned_subscriber.csv\")\n",
    "\n",
    "# Defining the correct column names\n",
    "subscriber_columns = [\"sub_id\", \"first_name\", \"last_name\", \"Street\", \"Birth_date\", \"Gender\", \"Phone\", \"Country\", \"City\", \"Zip_Code\", \"Subgrp_id\", \"Elig_ind\", \"eff_date\", \"term_date\"]\n",
    "\n",
    "# Reading the cleaned subscriber data from DBFS\n",
    "cleaned_subscriber_df = spark.read.csv(\"dbfs:/FileStore/tables/Cleaned_subscriber.csv\", header=True, inferSchema=True).toDF(*subscriber_columns)\n",
    "\n",
    "# Displaying the first few records to confirm the data\n",
    "cleaned_subscriber_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d97e0143-235d-41ef-b723-be8703323749",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----------+------------+----------+--------+----------------+--------------+----------+\n|Claim_Or_Rejected|    SUB_ID|claim_amount|claim_date|claim_id|      claim_type|  disease_name|patient_id|\n+-----------------+----------+------------+----------+--------+----------------+--------------+----------+\n|                N| SUBID1000|       79874|1949-03-14|       0| claims of value|  Galactosemia|    187158|\n|              NaN|SUBID10001|      151142|1970-03-16|       1|claims of policy|Bladder cancer|    112766|\n|              NaN|SUBID10002|       59924|2008-02-03|       2| claims of value| Kidney cancer|    199252|\n|              NaN|SUBID10003|      143120|1995-02-08|       3|  claims of fact|       Suicide|    133424|\n|                Y|SUBID10004|      168634|1967-05-23|       4| claims of value|  Food allergy|    172579|\n+-----------------+----------+------------+----------+--------+----------------+--------------+----------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Dataset: claims.json\n",
    "claims_df.write.format(\"json\").mode(\"overwrite\").save(\"dbfs:/FileStore/tables/Cleaned_claims.json\")\n",
    "\n",
    "# Reading the cleaned data from DBFS\n",
    "# Dataset: Cleaned_claims.json\n",
    "cleaned_claims_df = spark.read.json(\"dbfs:/FileStore/tables/Cleaned_claims.json\")\n",
    "cleaned_claims_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4782d49f-6acd-44ba-a3f8-62bb81e0bd88",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+\n|SubGrp_ID|Grp_Id|\n+---------+------+\n|     S101|GRP105|\n|     S102|GRP110|\n|     S102|GRP150|\n|     S102|GRP136|\n|     S103|GRP122|\n+---------+------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Dataset: grpsubgrp.csv\n",
    "group_subgroup_df.write.format(\"csv\").mode(\"overwrite\").save(\"dbfs:/FileStore/tables/Cleaned_grpsubgrp.csv\")\n",
    "\n",
    "# Defining the correct column names\n",
    "group_subgroup_columns = [\"SubGrp_ID\", \"Grp_Id\"]\n",
    "\n",
    "# Reading the cleaned group_subgroup data from DBFS\n",
    "cleaned_group_subgroup_df = spark.read.csv(\"dbfs:/FileStore/tables/Cleaned_grpsubgrp.csv\", header=True, inferSchema=True).toDF(*group_subgroup_columns)\n",
    "\n",
    "# Displaying the first few records to confirm the data\n",
    "cleaned_group_subgroup_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63cfa25d-02a7-4deb-b60e-22c1eba55352",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+\n|   disease_name|count|\n+---------------+-----+\n|    Pet allergy|    3|\n|        Anthrax|    3|\n|   Galactosemia|    3|\n|       Glaucoma|    3|\n|Phenylketonuria|    3|\n|   Head banging|    3|\n+---------------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# List of requirements\n",
    "# Which disease has a maximum number of claims\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Calculating the number of claims for each disease\n",
    "disease_counts = cleaned_claims_df.groupBy(\"disease_name\").count()\n",
    "\n",
    "# Finding the maximum number of claims\n",
    "max_claims_count = disease_counts.agg(F.max(\"count\")).collect()[0][0]\n",
    "\n",
    "# Finding all diseases with the maximum number of claims\n",
    "diseases_max_claims = disease_counts.filter(disease_counts[\"count\"] == max_claims_count)\n",
    "\n",
    "# Showing the result\n",
    "diseases_max_claims.show()\n",
    "\n",
    "# Saving as a permanent table\n",
    "permanent_table_name = \"diseases_max_claims\"\n",
    "diseases_max_claims.write.format(\"csv\").mode(\"overwrite\").saveAsTable(permanent_table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21ff801b-35bb-46b3-a05a-aa29bae651e1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+---------+---+---------+\n|    sub_id|  first_name|last_name|age|Subgrp_id|\n+----------+------------+---------+---+---------+\n|SUBID10017|      Bandhu|     Seth| 28|     S108|\n|SUBID10083|  Bhilangana|   Pandit| 29|     S109|\n|SUBID10093|Chandavarman|    Singh| 27|     S110|\n+----------+------------+---------+---+---------+\n\n"
     ]
    }
   ],
   "source": [
    "# Find those subscribers having age less than 30 and they subscribe any subgroup\n",
    "from pyspark.sql.functions import year, current_date\n",
    "\n",
    "# Calculating age\n",
    "cleaned_subscriber_df = cleaned_subscriber_df.withColumn(\"age\", year(current_date()) - year(cleaned_subscriber_df.Birth_date))\n",
    "\n",
    "# Subscribers with age less than 30\n",
    "subscribers_under_30 = cleaned_subscriber_df.filter(cleaned_subscriber_df.age < 30).select(\"sub_id\", \"first_name\", \"last_name\", \"age\", \"Subgrp_id\")\n",
    "subscribers_under_30.show()\n",
    "\n",
    "# Saving as a permanent table\n",
    "permanent_table_name = \"subscriber_under_30\"\n",
    "subscribers_under_30.write.format(\"csv\").mode(\"overwrite\").saveAsTable(permanent_table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0e65e10-f6fa-43b4-af7b-9b98bbf39e09",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n|Grp_Id|count|\n+------+-----+\n|GRP104|    2|\n|GRP147|    2|\n|GRP143|    2|\n+------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# Find out which group has maximum subgroups \n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Calculating the number of subgroups for each group\n",
    "group_counts = cleaned_group_subgroup_df.groupBy(\"Grp_Id\").count()\n",
    "\n",
    "# Finding the maximum number of subgroups\n",
    "max_subgroups_count = group_counts.agg(F.max(\"count\")).collect()[0][0]\n",
    "\n",
    "# Finding all groups with the maximum number of subgroups\n",
    "groups_max_subgroups = group_counts.filter(group_counts[\"count\"] == max_subgroups_count)\n",
    "\n",
    "# Showing the result\n",
    "groups_max_subgroups.show()\n",
    "\n",
    "# Saving as a permanent table\n",
    "permanent_table_name = \"groups_max_subgroups\"\n",
    "groups_max_subgroups.write.format(\"csv\").mode(\"overwrite\").saveAsTable(permanent_table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c16eaf3-c918-4d7c-9cdd-480ae98fda8d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n|hospital_id|count|\n+-----------+-----+\n|      H1017|    9|\n+-----------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# Find out hospital which serve most number of patients\n",
    "# The hospital which serves the most number of patients\n",
    "hospital_most_patients = cleaned_patients_df.groupBy(\"hospital_id\").count().orderBy(\"count\", ascending=False).limit(1)\n",
    "hospital_most_patients.show()\n",
    "\n",
    "# Saving as a permanent table\n",
    "permanent_table_name = \"hospital_most_patients\"\n",
    "hospital_most_patients.write.format(\"csv\").mode(\"overwrite\").saveAsTable(permanent_table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98c3b75a-ecd3-4349-8464-0150cc5054f5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n|Subgrp_id|count|\n+---------+-----+\n|     S104|   13|\n+---------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# Find out which subgroups subscribe most number of times\n",
    "# The subgroups which subscribe the most number of times\n",
    "subgroup_most_subscribed = cleaned_subscriber_df.groupBy(\"Subgrp_id\").count().orderBy(\"count\", ascending=False).limit(1)\n",
    "subgroup_most_subscribed.show()\n",
    "\n",
    "# Saving as a permanent table\n",
    "permanent_table_name = \"subgroup_most_subscribed\"\n",
    "subgroup_most_subscribed.write.format(\"csv\").mode(\"overwrite\").saveAsTable(permanent_table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d22f772-30d4-4958-8844-ccd5f662ce27",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----+\n|claim_or_rejected|count|\n+-----------------+-----+\n|                N|   22|\n+-----------------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# Find out total number of claims which were rejected\n",
    "# Grouping by 'claim_or_rejected' and counting the number of 'claim_id' for each group\n",
    "rejected_claims_count = cleaned_claims_df.groupBy(\"claim_or_rejected\").count()\n",
    "\n",
    "# Filtering to get only the rejected claims\n",
    "total_rejected_claims = rejected_claims_count.filter(rejected_claims_count[\"claim_or_rejected\"] == \"N\")\n",
    "\n",
    "# Showing the result\n",
    "total_rejected_claims.show()\n",
    "\n",
    "# Saving as a permanent table\n",
    "permanent_table_name = \"total_rejected_claims\"\n",
    "total_rejected_claims.write.format(\"csv\").mode(\"overwrite\").saveAsTable(permanent_table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d011377f-8bcf-4c79-ad4c-3921ffcfd1af",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n|        city|count|\n+------------+-----+\n|      Mysore|    2|\n|    Amravati|    2|\n|   Kamarhati|    2|\n|    Jabalpur|    2|\n|Bihar Sharif|    2|\n|   Ghaziabad|    2|\n|       Morbi|    2|\n|  Karimnagar|    2|\n+------------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# From where most claims are coming (city)\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Finding the city with the most claims\n",
    "city_claim_counts = cleaned_patients_df.join(claims_df, \"patient_id\").groupBy(\"city\").count()\n",
    "\n",
    "# Finding the maximum claim count\n",
    "max_claim_count = city_claim_counts.agg(F.max(\"count\")).first()[0]\n",
    "\n",
    "# Filtering cities that have the maximum claim count\n",
    "cities_most_claims = city_claim_counts.filter(city_claim_counts[\"count\"] == max_claim_count)\n",
    "\n",
    "# Showing the results\n",
    "cities_most_claims.show()\n",
    "\n",
    "# Saving as a permanent table\n",
    "permanent_table_name = \"cities_most_claims\"\n",
    "cities_most_claims.write.format(\"csv\").mode(\"overwrite\").saveAsTable(permanent_table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5efac9be-ddd4-45a2-80e6-c60acacb74d0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n|Grp_Type|count|\n+--------+-----+\n| Private|   51|\n|   Govt.|    7|\n+--------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# Which groups of policies subscriber subscribe mostly Government or private\n",
    "# Finding which groups of policies subscriber subscribe mostly Government or Private\n",
    "policy_type_most_subscribed = group_df.groupBy(\"Grp_Type\").count().orderBy(\"count\", ascending=False)\n",
    "policy_type_most_subscribed.show()\n",
    "\n",
    "# Saving as a permanent table\n",
    "permanent_table_name = \"policy_type_most_subscribed\"\n",
    "policy_type_most_subscribed.write.format(\"csv\").mode(\"overwrite\").saveAsTable(permanent_table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9e383a0-244d-45de-9844-ce6de2c77c77",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n|avg_monthly_premium|\n+-------------------+\n|             1870.0|\n+-------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Average monthly premium subscriber pay to insurance company\n",
    "from pyspark.sql.functions import avg\n",
    "\n",
    "# The average monthly premium subscriber pay to insurance company\n",
    "avg_monthly_premium = subgroup_df.agg({\"Monthly_Premium\": \"avg\"}).withColumnRenamed(\"avg(Monthly_Premium)\", \"avg_monthly_premium\")\n",
    "avg_monthly_premium.show()\n",
    "\n",
    "# Saving as a permanent table\n",
    "permanent_table_name = \"avg_monthly_premium\"\n",
    "avg_monthly_premium.write.format(\"csv\").mode(\"overwrite\").saveAsTable(permanent_table_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f5048c2-9f56-43a2-bb79-97ff76b5847e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+\n|Grp_Id|sum(premium_written)|\n+------+--------------------+\n|GRP131|               99000|\n|GRP123|               99000|\n|GRP147|               99000|\n+------+--------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Find out which group is most profitable\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Calculating total premium written for each group\n",
    "group_profitability = group_df.groupBy(\"Grp_Id\").sum(\"premium_written\")\n",
    "\n",
    "# Finding the maximum total premium written\n",
    "max_premium_written = group_profitability.agg(F.max(\"sum(premium_written)\")).first()[0]\n",
    "\n",
    "# Filtering all groups with the maximum total premium written\n",
    "most_profitable_groups = group_profitability.filter(group_profitability[\"sum(premium_written)\"] == max_premium_written)\n",
    "\n",
    "# Showing the results\n",
    "most_profitable_groups.show()\n",
    "\n",
    "# Saving as a permanent table\n",
    "permanent_table_name = \"most_profitable_groups\"\n",
    "most_profitable_groups.write.format(\"csv\").mode(\"overwrite\").saveAsTable(permanent_table_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8581681-dc42-4266-8486-8724c61a2580",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+--------------+------------------+-------------+------------+----+-----------+---+\n|Patient_id|Patient_name|patient_gender|patient_birth_date|patient_phone|disease_name|city|hospital_id|age|\n+----------+------------+--------------+------------------+-------------+------------+----+-----------+---+\n+----------+------------+--------------+------------------+-------------+------------+----+-----------+---+\n\n"
     ]
    }
   ],
   "source": [
    "# List all the patients below age of 18 who admit for cancer\n",
    "from pyspark.sql.functions import col, year, current_date\n",
    "\n",
    "# Calculating age based on current date\n",
    "cleaned_patients_df = cleaned_patients_df.withColumn(\"age\", year(current_date()) - year(cleaned_patients_df.patient_birth_date))\n",
    "\n",
    "# Filtering patients under 18 with diseases resembling cancer\n",
    "patients_under18_cancer = cleaned_patients_df.filter((col(\"age\") < 18) & (col(\"disease_name\").like(\"%cancer%\")))\n",
    "\n",
    "# Showing the result\n",
    "patients_under18_cancer.show()\n",
    "\n",
    "# Saving as a permanent table\n",
    "permanent_table_name = \"patients_under18_cancer\"\n",
    "patients_under18_cancer.write.format(\"csv\").mode(\"overwrite\").saveAsTable(permanent_table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b7bbdfd-d801-4c11-af84-fcac0b0d747f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------+------------------+------------+\n|patient_name|patient_gender|patient_birth_date|claim_amount|\n+------------+--------------+------------------+------------+\n+------------+--------------+------------------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "# List patients who have cashless insurance and have total charges greater than or equal for Rs. 50,000.\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Joining patients and claims DataFrames\n",
    "joined_df = cleaned_patients_df.join(cleaned_claims_df, cleaned_patients_df[\"Patient_id\"] == cleaned_claims_df[\"patient_id\"])\n",
    "\n",
    "# Applying filters for cashless insurance and claim amount >= 50000\n",
    "filtered_df = joined_df.filter((cleaned_claims_df[\"claim_amount\"] >= 50000) & (cleaned_claims_df[\"claim_type\"] == 'Cashless'))\n",
    "\n",
    "# Selecting required columns from the joined DataFrame\n",
    "patients_cashless_chargegreater = filtered_df.select(\"patient_name\", \"patient_gender\", \"patient_birth_date\", \"claim_amount\")\n",
    "\n",
    "# Showing the result\n",
    "patients_cashless_chargegreater.show()\n",
    "\n",
    "# Saving as a permanent table\n",
    "permanent_table_name = \"patients_cashless_chargegreater\"\n",
    "patients_cashless_chargegreater.write.format(\"csv\").mode(\"overwrite\").saveAsTable(permanent_table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ca6e3ae-9bd9-49f6-912d-7d4c28aa739a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+--------------+------------------+------------+----+-----------+\n|Patient_id|Patient_name|patient_gender|patient_birth_date|disease_name|city|hospital_id|\n+----------+------------+--------------+------------------+------------+----+-----------+\n+----------+------------+--------------+------------------+------------+----+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "# List female patients over the age of 40 that have undergone knee surgery in the past year\n",
    "from pyspark.sql.functions import col, year, current_date, to_date, expr\n",
    "\n",
    "# Calculating age based on current date\n",
    "cleaned_patients_df = cleaned_patients_df.withColumn(\"age\", year(current_date()) - year(cleaned_patients_df.patient_birth_date))\n",
    "\n",
    "# Filtering female patients over 40\n",
    "female_patients_over_40 = cleaned_patients_df.filter((col(\"age\") > 40) & (col(\"patient_gender\") == \"Female\"))\n",
    "\n",
    "# Calculating date one year ago from current date\n",
    "one_year_ago = current_date() - expr(\"INTERVAL 1 YEAR\")\n",
    "\n",
    "# Joining with claims DataFrame to find knee surgery patients in the past year\n",
    "knee_surgery_patients = female_patients_over_40.join(\n",
    "    cleaned_claims_df,\n",
    "    (female_patients_over_40[\"Patient_id\"] == cleaned_claims_df[\"patient_id\"]) &  # Specifying DataFrame for each column\n",
    "    (cleaned_claims_df[\"disease_name\"].like(\"%knee surgery%\")) &\n",
    "    (to_date(cleaned_claims_df[\"claim_date\"], 'yyyy-MM-dd') >= one_year_ago)\n",
    ").select(\n",
    "    female_patients_over_40[\"Patient_id\"], \n",
    "    female_patients_over_40[\"Patient_name\"],\n",
    "    female_patients_over_40[\"patient_gender\"],\n",
    "    female_patients_over_40[\"patient_birth_date\"],\n",
    "    cleaned_claims_df[\"disease_name\"],\n",
    "    female_patients_over_40[\"city\"],\n",
    "    female_patients_over_40[\"hospital_id\"]\n",
    ")\n",
    "\n",
    "# Showing the result\n",
    "knee_surgery_patients.show()\n",
    "\n",
    "# Saving as a permanent table\n",
    "permanent_table_name = \"knee_surgery_patients\"\n",
    "knee_surgery_patients.write.format(\"csv\").mode(\"overwrite\").saveAsTable(permanent_table_name)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Health Care Insurance Data Analysis",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
